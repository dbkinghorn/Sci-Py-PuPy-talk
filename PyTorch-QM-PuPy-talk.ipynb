{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <p align=\"center\">Scientific Computing with PyTorch<br> A Quantum Mechanics Example</p>\n",
    "## <p align=\"center\">Dr. Donald B Kinghorn<br> [Puget Systems](https://www.pugetsystems.com)\n",
    "\n",
    "\n",
    "### <p align=\"center\">A Quantum Mechanics problem coded up in PyTorch?! Sure! Why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reference Blog Posts @ [https://www.pugetsystems.com/all_hpc.php](https://www.pugetsystems.com/all_hpc.php)\n",
    "\n",
    "* #### [Doing Quantum Mechanics with a Machine Learning Framework: PyTorch and Correlated Gaussian Wavefunctions: Part 1) Introduction](https://www.pugetsystems.com/labs/hpc/Doing-Quantum-Mechanics-with-a-Machine-Learning-Framework-PyTorch-and-Correlated-Gaussian-Wavefunctions-Part-1-Introduction-1207/)\n",
    "* #### [PyTorch for Scientific Computing - Quantum Mechanics Example Part 2) Program Before Code Optimizations](https://www.pugetsystems.com/labs/hpc/PyTorch-for-Scientific-Computing---Quantum-Mechanics-Example-Part-2-Program-Before-Code-Optimizations-1222/)\n",
    "* #### [PyTorch for Scientific Computing - Quantum Mechanics Example Part 3) Code Optimizations - Batched Matrix Operations, Cholesky Decomposition and Inverse](https://www.pugetsystems.com/labs/hpc/PyTorch-for-Scientific-Computing---Quantum-Mechanics-Example-Part-3-Code-Optimizations---Batched-Matrix-Operations-Cholesky-Decomposition-and-Inverse-1225/)\n",
    "* #### [PyTorch for Scientific Computing - Quantum Mechanics Example Part 4) Full Code Optimizations -- 16000 times faster on a Titan V GPU](https://www.pugetsystems.com/labs/hpc/PyTorch-for-Scientific-Computing---Quantum-Mechanics-Example-Part-4-Full-Code-Optimizations----16000-times-faster-on-a-Titan-V-GPU-1230/)\n",
    "\n",
    "\n",
    "* ##### New to PyTorch?  [Why You Should Consider PyTorch (includes Install and a few examples)](https://www.pugetsystems.com/labs/hpc/Why-You-Should-Consider-PyTorch-includes-Install-and-a-few-examples-1193/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why PyTorch?\n",
    "\n",
    "* PyTorch is compatible with Numpy and has equivalents for most of it's routines\n",
    "* Switching between CPU and CUDA on GPU is seamless and very simple.\n",
    "* Automatic differentiation is well implemented and relatively easy to use\n",
    "* PyTorch contains a rich set of both CPU and CUDA based BLAS (Basic Linear Algebra Subroutines) and Lapack (higher level linear algebra algorithms) Intel MKL, cuBLAS and MAGMA.\n",
    "* It has \"batched\" routines to extend matrix operation to larger Tensors structures.\n",
    "* It's interactive and easy to use in a Jupyter notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Quantum Mechanics\n",
    "\n",
    "Here is your brief introduction to doing quantum mechanics of few particle systems from \"first principles\".\n",
    "\n",
    "We want to solve the Schrodinger equation,\n",
    "\n",
    "$$\n",
    "H\\Psi = E\\Psi\n",
    "$$\n",
    "\n",
    "$H$ is the Hamiltonian energy operator, $E$ is an energy eigenvalue and $\\Psi$ is an eigenfunction of $H$, the \"wavefunction\" of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notes:\n",
    " The Schrodinger equation is separable into time-dependent and time-independent parts. We look at the time-independent part that governs the internal states of a system.\n",
    "\n",
    " We use natural \"au\" units,\n",
    " * $\\hbar = 1$\n",
    " * Mass of electron = 1\n",
    " * electron charge is -1\n",
    " * Energy is in units of \"hartrees\"\n",
    "\n",
    "Our model system is a collection of particles with given masses and charges interacting under a coulomb potential with no external fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " You can turn the problem into a numerical partial differential equation (Matrix eigenvalue problem) by expanding $\\Psi$ in a \"basis set\" of appropriate functions.\n",
    "\n",
    "  $$\n",
    "  \\Psi = \\sum_k c_k \\phi_k(A_k;r)\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Hamiltonian Operator\n",
    "   \"internal\" coordinates after removing translational energy of center of mass (3 degrees of freedom), an exact transformation.\n",
    "\n",
    "$H$ is the sum of kinetic $T$ and potential $V$ energy operators,\n",
    "\n",
    "$$\n",
    "H = -\\frac 12\\left( \\sum_i^n\\frac 1{\\mu_i}\\nabla_i^2+\\sum_{i,j}^n\\frac\n",
    "1{M_1}\\nabla_i\\cdot \\nabla_j\\right) + \\sum_i^n\\frac {q_0q_i}{r_i} + \\sum_i^n \\sum_{i<j} \\frac {q_iq_j}{r_{ij}}\n",
    "$$\n",
    "\n",
    "\n",
    "* $\\{M_1,\\cdots ,M_p\\}$ are particle masses\n",
    "* $\\mu_i=M_1M_i/\\left( M_1+M_i\\right) $ are the reduced masses, $q_i$ are the particle charges \n",
    "* $r_{ij}=\\left\\| r_i-r_j\\right\\| $\n",
    "* $r_i$ is the Cartesian coordinate vector for particle $i$\n",
    "* $\\nabla_i$ is the gradient operator with respect to the $x,y,z$ coordinates of particle $i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Basis Set (correlated Gaussians)\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\phi_k &=&\\exp \\left[ -r^{\\prime }\\left( L_kL_k^{\\prime }\\otimes I_3\\right)\n",
    "r\\right]\n",
    "&=&\\exp \\left[ -r^{\\prime }\\left( A_k\\otimes I_3\\right) r\\right]\n",
    "&=&\\exp \\left[ -r^{\\prime }\\bar{A}_kr\\right]\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "* $r$ is a $3n \\times 1$ vector of Cartesian coordinates for the $n$ particles\n",
    "* $L_k$ is an $n\\times n$ rank $n$ lower triangular matrix\n",
    "* $A_k=L_kL_k^{\\prime }$ is written in this Cholesky factored form\n",
    "* $\\exp \\left[ -r^{\\prime\n",
    "}\\left( A_k\\otimes I_3\\right) r\\right] =\\exp \\left[\n",
    "-\\sum_{i,j}a_{ij}r_i\\cdot r_j\\right] $\n",
    "* $\\Psi = \\sum_k c_k \\phi_k$\n",
    "\n",
    "The Kronecker product with the $3\\times 3$ identity matrix $I_3$ insures rotational invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The end result of the discussion above is that we need to find \"matrix elements\" for $H$ and for the \"overlap\" $S$ and then minimize the smallest eigenvalue of the generalized  eigenvalue equation, \n",
    "\n",
    "$$Hc=ESc$$ $H$ and $S$ are matrices of integrals like,\n",
    "\n",
    "$$\n",
    "H_{kl} = \\langle\\phi_k|H|\\phi_l\\rangle = \\int^\\infty_{-\\infty} \\phi^\\star_k H \\phi_l dr\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_{kl} = \\langle\\phi_k|\\phi_l\\rangle = \\int^\\infty_{-\\infty} \\phi^\\star_k  \\phi_l dr\n",
    "$$\n",
    "\n",
    "$\\langle\\Phi|O|\\Phi\\rangle$ is a probabilistic \"expectation value\" for measurements represented by the operator $O$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Symmetry\n",
    "There is one more detail, permutational symmetry and \"spin\".\n",
    "\n",
    "* The Hamiltonian operator is invariant to interchange of like particles.\n",
    "\n",
    "* One of the defining aspects of quantum mechanics is the symmetry of the wavefunction that is induced by spin angular momentum (particularly the half integral spin of electrons).\n",
    "\n",
    "It is consideration of the symmetry aspects of the system that defines the valid \"particular solutions\" to the Schrodinger equation.\n",
    "\n",
    "We need a proper symmetry projection operator to get the wavefunction we are looking for.\n",
    "\n",
    "We are doing close to \"pure\" quantum mechanics with very few approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## You now know the fundamentals of \"doing\" quantum mechanics. Congratulations!\n",
    "\n",
    "#### Now on to the formulas and code ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The formulas that need to be coded up\n",
    "We need to find the integral formulas for $H$ and $S$ over those basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Overlap $S$\n",
    "\n",
    "$$\n",
    "S_{kl}= 2^{3n/2}\\left( \\frac{\\left\\|\n",
    "L_k\\right\\| \\left\\| L_l\\right\\| }{\\left| A_{kl}\\right| }\\right) ^{3/2}\n",
    "$$\n",
    "$L_k$ and $L_l$ are lower triangular matrices of of (independent variable) parameters that will be optimized. $A_{kl} = A_k + \\tau_P^{\\prime }A_l\\tau_P$ with $\\tau$ a symmetry projection matrix. $|A|$ means determinate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Kinetic energy $T$\n",
    "\n",
    "$$\n",
    "T_{kl}=6S_{kl}\\,\\mathrm{tr}\\left[ MA_kA_{kl}^{-1}\\tau_P^{\\prime }A_l\\tau_P\\right]\n",
    "$$\n",
    "\n",
    "$M$ is a matrix of reduced mass constants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Potential energy $V$\n",
    "\n",
    "$$\n",
    "V_{kl}=\\frac 2{\\sqrt{\\pi }}S_{kl}\\, \\sum_{i>j}Q*R_{ij}\n",
    "$$\n",
    "\n",
    "$Q$ is a lower triangular matrix of \"charge products\" and $R_{ij}$ is a lower triangular matrix constructed from elements in $A_{kl}^{-1}$\n",
    "$$\n",
    "R_{ij}=\\,\\mathrm{\\,\\,} \\,\\left(\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\left[A_{kl}^{-1}\\right]_{ii}^{-1/2} & i=j \\\\\n",
    "\\left(\\left[A_{kl}^{-1}\\right]_{ii}+\\left[A_{kl}^{-1}\\right]_{jj} -2\\left[A_{kl}^{-1}\\right]_{ij}  \\right)^{-1/2} & i>j\n",
    "\\end{array}\n",
    "\\right. \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Important Point!\n",
    "**Those are very compact matrix equations for what are really quite elaborate integrals derived with matrix calculus.** If those formulas were written out with summation signs instead of matrix formulas there would be several pages of ugly formulas to consider.\n",
    "\n",
    "**Having formulas in matrix form is a big plus for code implementation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A really good book ...\n",
    "### matrix differential calculus with applications in statistics and econometrics\n",
    "\n",
    "#### Jan R. Magnus, Heinz Neudecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Energy function\n",
    "For the energy (loss) function we use the \"Rayleigh quotient\",\n",
    "\n",
    "$$\n",
    "E = \\min_c\\left(\\frac{c'Hc}{c'Sc} \\right)  \n",
    "$$\n",
    "\n",
    "The minimum over $c$ of the Rayleigh quotient is the smallest eigenvalue and that minimal $c$ is the corresponding eigenvector.\n",
    "\n",
    "By doing this we can optimize the linear expansion coefficients in $c$ simultaneously with the non-linear parameters that are contained in the matrices $L_k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The independent parameter variables for our energy (loss) function $E$ is the collection of all of the lower triangular elements of the $L_k$ matrices for all of the basis functions along with  the vector of expansion coefficients $c$. Those optimization parameters will be placed in a vector x. That will give $E$ as a function of x that will return the energy and gradient at the value of x. We will minimize that.\n",
    "\n",
    "**Got it? Let's code it up!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First \"naive\" implementation of the code in PyTorch\n",
    "\n",
    "This is a completely reasonable implementation of the equations. It was translated from my old MATLAB testing code. It is almost directly going from matrix-math to matrix-code.\n",
    "\n",
    "\n",
    ">**The first thing to do is get working code!**\n",
    "\n",
    "(there is test code in the blog post \"part 2\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch as th # PyTorch is imported as torch and will be referenced as \"th\"\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Setup compute device and data-type\n",
    "We will use the CPU until we do the code optimizations. (The code as-is will run on the GPU but performance is bad.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution device:  cpu\n",
      "PyTorch version:  1.0.1\n",
      "CUDA available:  True\n",
      "CUDA version:  10.0\n",
      "CUDA device: GeForce GTX 1060 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "dtype = th.float64\n",
    "\n",
    "gpuid = 0\n",
    "#device = th.device(\"cuda:\"+ str(gpuid))\n",
    "device = th.device(\"cpu\")\n",
    "\n",
    "print(\"Execution device: \",device)\n",
    "print(\"PyTorch version: \", th.__version__ )\n",
    "print(\"CUDA available: \", th.cuda.is_available())\n",
    "print(\"CUDA version: \", th.version.cuda)\n",
    "print(\"CUDA device:\", th.cuda.get_device_name(gpuid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Utility functions for working with lower triangular matrices\n",
    "\n",
    "# return the lower triangle of A in column order i.e. vech(A)\n",
    "def vech(A):\n",
    "    count = 0\n",
    "    c = A.shape[0]\n",
    "    v = th.zeros(c * (c + 1) // 2, device=device, dtype=dtype)\n",
    "    for j in range(c):\n",
    "        for i in range(j,c):\n",
    "            v[count] = A[i,j]\n",
    "            count += 1\n",
    "    return v\n",
    "\n",
    "# vech2L   create lower triangular matrix L from vechA\n",
    "def vech2L(v,n):\n",
    "    count = 0\n",
    "    L = th.zeros((n,n), device=device, dtype=dtype)\n",
    "    for j in range(n):\n",
    "        for i in range(j,n):\n",
    "            L[i,j]=v[count]\n",
    "            count += 1\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Compute formulas for \"matrix elements\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def matrix_elements(n, vechLk, vechLl, Sym, Mass, vecQ):\n",
    "    '''\n",
    "    Returns: a dictionary with the skl, tkl, vkl matrix elements\n",
    "    n : the size of the Lk matrices\n",
    "    vechL(k,l): vector of parameters use for Lk and Ll matrices\n",
    "    Sym: a single symmetry projection matrix\n",
    "    Mass: a matrix of \"reduced mass\" values for the particles needed for tkl\n",
    "    vecQ: an array of products of particle charges needed for vkl   \n",
    "    '''\n",
    "    # build Lk and Ll\n",
    "\n",
    "    Lk = vech2L(vechLk,n);\n",
    "    Ll = vech2L(vechLl,n);\n",
    "\n",
    "    # apply symmetry projection on Ll\n",
    "\n",
    "    # th.t() is shorthand for th.transpose(X, 0,1)\n",
    "    PLl = th.t(Sym) @ Ll;\n",
    "\n",
    "    # build Ak, Al, Akl, invAkl\n",
    "\n",
    "    Ak = Lk@th.t(Lk);\n",
    "    Al = PLl@th.t(PLl);\n",
    "    Akl = Ak+Al\n",
    "    invAkl = th.inverse(Akl);\n",
    "\n",
    "    # Overlap (normalized)\n",
    "    skl = 2**(3*n/2) * th.sqrt( th.pow(th.abs(th.det(Lk))*th.abs(th.det(Ll))/th.det(Akl) ,3) );\n",
    "\n",
    "    # Kinetic energy\n",
    "\n",
    "    tkl = skl*(6*th.trace(Mass@Ak@invAkl@Al));\n",
    "\n",
    "    # potential energy\n",
    "\n",
    "    TWOoSqrtPI = 1.1283791670955126 # 2/sqrt(pi)\n",
    "\n",
    "    RIJ = th.zeros((n,n), device=device, dtype=dtype);\n",
    "    # 1/rij i~=j\n",
    "    for j in range(0,n-1):\n",
    "        for i in range(j+1,n):\n",
    "            tmp2 = invAkl[i,i] + invAkl[j,j] - 2*invAkl[i,j];\n",
    "            #RIJ[i,j] = TWOoSqrtPI * skl/th.sqrt(tmp2);\n",
    "            RIJ[i,j] = 1/th.sqrt(tmp2)\n",
    "\n",
    "    # 1/rij i=j\n",
    "    for i in range(0,n):\n",
    "        #RIJ[i,i] = TWOoSqrtPI * skl/th.sqrt(invAkl[i,i]);\n",
    "        RIJ[i,i] = 1/th.sqrt(invAkl[i,i])\n",
    "\n",
    "    RIJ = TWOoSqrtPI*skl*RIJ    \n",
    "    Q = vech2L(vecQ,n);\n",
    "\n",
    "    vkl = th.sum(RIJ*Q)\n",
    "\n",
    "    return {'skl':skl, 'tkl':tkl, 'vkl':vkl}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You can see how close the PyTorch/Python code is to the mathematical matrix formulas. This makes it very quick to get working code running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Define the energy \"loss\" function\n",
    "\"energy()\" loops over the lower triangle of S T and V making calls to the matrix element code. These matrices are then used to create the Hamiltonian matrix H and Overlap S that are then used to compute the energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def energy(x,n,nb,Mass,Charge,Sym,symc):\n",
    "    '''\n",
    "    Returns: the energy at the point (parameters) x\n",
    "    n: number of particles defined in input\n",
    "    nb: number of basis functions\n",
    "    Mass: matrix of \"reduced\" mass constants for system\n",
    "    Charge: vector of \"charge products\" for particle pairs\n",
    "    Sym: a tensor containing the symmetry projection matrices\n",
    "    symc: a vector of coefficients for the symmetry projection terms\n",
    "    '''\n",
    "\n",
    "    nx = len(x);\n",
    "    nn = int(n*(n+1)/2);\n",
    "    nsym = len(symc);\n",
    "\n",
    "    # extract linear coefs \"eigen vector\" from x\n",
    "    c = x[-nb:];\n",
    "    # reshape non-linear variables for easier indexing\n",
    "    X = th.reshape(x[:nb*nn], (nb,nn))\n",
    "\n",
    "    # Init H S T V\n",
    "    H = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    S = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    T = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    V = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "\n",
    "    # outer loop is over symmetry terms\n",
    "    for k in range(0,nsym):\n",
    "\n",
    "        for j in range(0,nb):\n",
    "            for i in range(j,nb):\n",
    "\n",
    "                vechLi = X[i,:]\n",
    "                vechLj = X[j,:]\n",
    "\n",
    "                matels = matrix_elements(n,vechLi,vechLj,Sym[:,:,k],Mass,Charge);\n",
    "\n",
    "                S[i,j] += symc[k]*matels['skl'];\n",
    "                T[i,j] += symc[k]*matels['tkl'];\n",
    "                V[i,j] += symc[k]*matels['vkl'];\n",
    "\n",
    "    H = T + V\n",
    "\n",
    "    # complete upper triangle of H and S\n",
    "    for i in range(0,nb):\n",
    "        for j in range(i+1,nb):\n",
    "            H[i,j] = H[j,i];\n",
    "            S[i,j] = S[j,i];\n",
    "\n",
    "    # The energy from the Rayleigh quotient\n",
    "\n",
    "    cHc = c@H@c;\n",
    "    cSc = c@S@c;\n",
    "    eng = cHc/cSc;\n",
    "\n",
    "    return eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Optimize energy using PyTorch Autograd gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def opt_energy(steps=1, num_basis=8):\n",
    "\n",
    "    ############################################################################\n",
    "    # Input constants for Li atom (infinite nuclear mass) Using B.O. approx.\n",
    "    # in order to compare with other calculations. You can use actual isotopic\n",
    "    # mass values for Li here. Mass would then have off-diag terms. See reduced\n",
    "    # mass derivation.\n",
    "    ############################################################################\n",
    "    Mass = th.tensor([[0.5, 0.0, 0.0],\n",
    "                     [0.0, 0.5, 0.0],\n",
    "                     [0.0, 0.0, 0.5]], device=device, dtype=dtype);\n",
    "\n",
    "    Charge = th.tensor([-3, 1, 1, -3, 1, -3], device=device, dtype=dtype);\n",
    "\n",
    "    # symmetry projection terms\n",
    "    Sym = th.zeros((3,3,6), device=device, dtype=dtype)\n",
    "    # (1)(2)(3)\n",
    "    Sym[:,:,0] = th.tensor([[1,0,0],[0,1,0],[0,0,1]], device=device, dtype=dtype);\n",
    "    # (12)\n",
    "    Sym[:,:,1] = th.tensor([[0,1,0],[1,0,0],[0,0,1]], device=device, dtype=dtype);\n",
    "    # (13)\n",
    "    Sym[:,:,2] = th.tensor([[0,0,1],[0,1,0],[1,0,0]], device=device, dtype=dtype);\n",
    "    # (23)\n",
    "    Sym[:,:,3] = th.tensor([[1,0,0],[0,0,1],[0,1,0]], device=device, dtype=dtype);\n",
    "    # (123)\n",
    "    Sym[:,:,4] = th.tensor([[0,1,0],[0,0,1],[1,0,0]], device=device, dtype=dtype);\n",
    "    # (132)\n",
    "    Sym[:,:,5] = th.tensor([[0,0,1],[1,0,0],[0,1,0]], device=device, dtype=dtype);\n",
    "\n",
    "    # coeff's\n",
    "    symc = th.tensor([4.0,4.0,-2.0,-2.0,-2.0,-2.0], device=device, dtype=dtype);\n",
    "    ############################################################################\n",
    "\n",
    "\n",
    "    n=3\n",
    "    nb=num_basis\n",
    "    th.manual_seed(3)\n",
    "    x1 = th.empty(int(nb*n*(n+1)/2 + nb), device=device, dtype=dtype, requires_grad=True)\n",
    "    th.nn.init.uniform_(x1, a=-.8, b=.8)    \n",
    "\n",
    "    #x1 = xrestart\n",
    "\n",
    "    optimizer = th.optim.Rprop([x1], lr=0.001, etas=(0.5, 1.2), step_sizes=(1e-06, 50))\n",
    "\n",
    "    for i in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        loss = energy(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "        loss.backward()      \n",
    "        optimizer.step()\n",
    "\n",
    "        if (i<10 or not i%10):print('step: {:5}  f: {:4.12f}  gradNorm: {:.9f}'.format(i, loss, th.norm(x1.grad)))\n",
    "    print('step: {:5}  f: {:4.12f}  gradNorm: {:.9f}'.format(i, loss, th.norm(x1.grad)))\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The gradient for the optimization is evaluated by \"back propagation\" in the statement loss.backward(). I used the name loss in honor of standard Machine Learning name for the function being minimized.\n",
    "\n",
    "I experimented with some of the machine learning optimization routines in PyTorch and found that \"Rprop\" worked really well for this problem. I used the a utility from the neural network module \"nn\" for the random parameter initialization. It was great having these kinds of tools in the framework for experimenting with!\n",
    "\n",
    "You can run this code and it will optimize a wavefunction for the Li atom ground state energy. It will be converging toward a value of -7.478060 Hartrees' (an energy unit typically used in quantum chemistry). Running this for 100 \"epochs\" i.e. optimization steps gives the following output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:     0  f: -0.782443141610  gradNorm: 6.657278389\n",
      "step:     1  f: -0.813405917937  gradNorm: 6.663333560\n",
      "step:     2  f: -0.850565814281  gradNorm: 6.664066389\n",
      "step:     3  f: -0.895095638316  gradNorm: 6.655362604\n",
      "step:     4  f: -0.948325979569  gradNorm: 6.631036771\n",
      "step:     5  f: -1.011714042305  gradNorm: 6.582179512\n",
      "step:     6  f: -1.086766895226  gradNorm: 6.496718349\n",
      "step:     7  f: -1.174915455717  gradNorm: 6.359865988\n",
      "step:     8  f: -1.277512901528  gradNorm: 6.162854762\n",
      "step:     9  f: -1.395322273794  gradNorm: 5.891110020\n",
      "step:    10  f: -1.528494111693  gradNorm: 5.568453647\n",
      "step:    19  f: -3.639937664856  gradNorm: 3.017759236\n",
      " took 11.707125186920166 seconds \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "xrestart = opt_energy(steps=20, num_basis=8)\n",
    "print(\" took {} seconds \".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The next run is for a wavefunction with 512 basis terms. This is one step of the optimization which will give us a time for one energy and gradient evaluation. (this is from a random start point) This is output from my 14-core Xeon-W 2175 system.\n",
    "```\n",
    "start_time = time.time()\n",
    "xrestart = opt_energy(steps=1, num_basis=512)\n",
    "print(\" took {} seconds \".format(time.time() - start_time))\n",
    "```\n",
    "```\n",
    "step:     0  f: -0.720436686340  gradNorm: 4.060212842\n",
    " took 2768.9312148094177 seconds\n",
    "```\n",
    "2768 seconds for one step is too long!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Interlude\n",
    "## constant time scaling on GPU!\n",
    "### tensor algorithms and batched operations are your friend on GPU\n",
    "\n",
    "This stuff is from \"part 3\" of the blog posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Batched Cholesky Decomposition in PyTorch\n",
    "\n",
    "**This is the most important concept in these slides.**\n",
    "\n",
    "Lets say you have many, $n \\times n$ matrices, $m$ of them. You want to do an algorithm that acts on the individual elements of those matrices i.e. like a Cholesky decomposition and maybe find determinants and inverses. For a data structure to hold these matrices you can use an $ m \\times n \\times n$ tensor. You can visualize that like this,\n",
    "\n",
    "![Tensor box](tensor-box.png)\n",
    "\n",
    "Think of that as a stack of matrices. Each i,j matrix element can be considered a 1 x 1 x m \"column\" vector going through the whole stack. If you build an algorithm that acts on on the i,j elements of a matrix it is trivial to extend that to acting on the whole \"column\" vector going through that stack at once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Matrix version of Cholesky decomposition (in PyTorch)\n",
    "Here's a Python implementation acting on matrices. \n",
    "[Note: I am not doing any input \"sanity\" checking in this code.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Matrix Cholesky decomp\n",
    "def matrix_cholesky(A):\n",
    "    L = th.zeros_like(A)\n",
    "\n",
    "    for i in range(A.shape[-1]):\n",
    "        for j in range(i+1):\n",
    "            s = 0.0\n",
    "            for k in range(j):\n",
    "                s = s + L[i,k] * L[j,k]\n",
    "\n",
    "            L[i,j] = th.sqrt(A[i,i] - s) if (i == j) else \\\n",
    "                      (1.0 / L[j,j] * (A[i,j] - s))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Batched tensor version of Cholesky decomposition (in PyTorch)\n",
    "\n",
    "Now for the batched version. This is a trivial modification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Batched Cholesky decomp\n",
    "def batch_cholesky(A):\n",
    "    L = th.zeros_like(A)\n",
    "\n",
    "    for i in range(A.shape[-1]):\n",
    "        for j in range(i+1):\n",
    "            s = 0.0\n",
    "            for k in range(j):\n",
    "                s = s + L[...,i,k] * L[...,j,k]\n",
    "\n",
    "            L[...,i,j] = th.sqrt(A[...,i,i] - s) if (i == j) else \\\n",
    "                      (1.0 / L[...,j,j] * (A[...,i,j] - s))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**All that was need to make a batched version was to add [..., ] as first index into the tensor!**\n",
    " That works for both stacked and tiled structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Modification to allow \"Autograd\" Gradients (in PyTorch)\n",
    "**If you want to include these operation in autograd for gradients in the computation graph you can not have any \"in-place\" operations.** The code above modifies L in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Batched Cholesky decomp (Autograd safe)\n",
    "def cholesky(A):\n",
    "    L = th.zeros_like(A)\n",
    "\n",
    "    for i in range(A.shape[-1]):\n",
    "        for j in range(i+1):\n",
    "            s = 0.0\n",
    "            for k in range(j):\n",
    "                s = s + L[...,i,k].clone() * L[...,j,k].clone()\n",
    "\n",
    "            L[...,i,j] = th.sqrt(A[...,i,i] - s) if (i == j) else \\\n",
    "                      (1.0 / L[...,j,j].clone() * (A[...,i,j] - s))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Batched Cholesky testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution device:  cuda:0\n",
      "PyTorch version:  1.0.1\n",
      "CUDA version:  10.0\n",
      "CUDA device: GeForce GTX 1060 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "dtype = th.float32\n",
    "\n",
    "gpuid = 0\n",
    "device = th.device(\"cuda:\"+ str(gpuid))\n",
    "#device = th.device(\"cpu\")\n",
    "\n",
    "print(\"Execution device: \",device)\n",
    "print(\"PyTorch version: \", th.__version__ )\n",
    "print(\"CUDA version: \", th.version.cuda)\n",
    "print(\"CUDA device:\", th.cuda.get_device_name(gpuid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "# Batched vech2L input V is nb x n(n+1)/2\n",
    "def bvech2L(V,nb,n):\n",
    "    count = 0\n",
    "    L = th.zeros((nb,n,n), device=device, dtype=dtype)\n",
    "    for j in range(n):\n",
    "        for i in range(j,n):\n",
    "            L[...,i,j]=V[...,count]\n",
    "            count = count + 1\n",
    "    return L\n",
    "\n",
    "# Batched Cholesky decomp\n",
    "def cholesky(A):\n",
    "    L = th.zeros_like(A)\n",
    "\n",
    "    for i in range(A.shape[-1]):\n",
    "        for j in range(i+1):\n",
    "            s = 0.0\n",
    "            for k in range(j):\n",
    "                s = s + L[...,i,k].clone() * L[...,j,k].clone()\n",
    "\n",
    "            L[...,i,j] = th.sqrt(A[...,i,i] - s) if (i == j) else \\\n",
    "                      (1.0 / L[...,j,j].clone() * (A[...,i,j] - s))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a couple of functions to do the testing,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(n=4,m=1000):\n",
    "\n",
    "    nn = int(n*(n+1)/2)\n",
    "\n",
    "    th.manual_seed(42)\n",
    "    X = th.rand((m,nn), device=device, dtype=dtype)\n",
    "    L = th.add(bvech2L(X,m,n),th.eye(n, device=device, dtype=dtype))\n",
    "    A = th.matmul(L,th.transpose(L, 1, 2))\n",
    "    print(\"Shape of A {}\".format(A.shape))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    cholA = th.zeros_like(A)\n",
    "    for i in range(m):\n",
    "        #cholA[i,:,:] = th.potrf(A[i], upper=False)\n",
    "        cholA[i,:,:] = th.cholesky(A[i], upper=False)\n",
    "\n",
    "    runtime = time.time() - start_time\n",
    "    print(\"loop version took {} seconds \".format(runtime))\n",
    "    return runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(n=4,m=1000):\n",
    "\n",
    "    nn = int(n*(n+1)/2)\n",
    "\n",
    "    th.manual_seed(42)\n",
    "    X = th.rand((m,nn), device=device, dtype=dtype)\n",
    "    L = th.add(bvech2L(X,m,n), th.eye(n, device=device, dtype=dtype))\n",
    "    A = th.matmul(L,th.transpose(L, 1, 2))\n",
    "    print(\"Shape of A {}\".format(A.shape))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    cholA = th.zeros_like(A)\n",
    "\n",
    "    cholA = cholesky(A)\n",
    "\n",
    "    runtime = time.time() - start_time\n",
    "    print(\"batched version took {} seconds \".format(runtime))\n",
    "    return runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A torch.Size([1000, 10, 10])\n",
      "loop version took 3.6238770484924316 seconds \n"
     ]
    }
   ],
   "source": [
    "tval = test_loop(n=10,m=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A torch.Size([1000, 10, 10])\n",
      "batched version took 0.06857466697692871 seconds \n"
     ]
    }
   ],
   "source": [
    "tval = test_batch(n=10,m=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A torch.Size([1000000, 10, 10])\n",
      "batched version took 0.10876941680908203 seconds \n"
     ]
    }
   ],
   "source": [
    "tval = test_batch(n=10,m=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1,000,000 10 x 10 matrices (batched has no slowdown! - 0.0126 sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**This GPU result is one of the most amazing things I've ever seen on a compute device! The calculation actually takes less time to run on 1 million matrices than it does on 10 The compute time is essentially constant no matter how many matrices are in the batch!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following plots were produced on this system ...\n",
    "I'm running on my personal [Puget Systems \"Peak Single Xeon Tower\"](https://www.pugetsystems.com/peak.php) which is configured with;\n",
    "* Intel Xeon-W 2175 14-core\n",
    "* 128 GB DDR4 2600MHz memory\n",
    "* NVIDIA 1080Ti\n",
    "* NVIDIA Titan V (used for compute)\n",
    "\n",
    "### Performance plots looped vs batched Cholesky  on CPU and GPU\n",
    "\n",
    "#### Looped vs Batched CPU and GPU\n",
    "\n",
    "The first plot is 1 to 10,000 (10 x 10) matrices running on CPU. Looped and batched are both reasonably fast on CPU but there is better performance with the batched code as the number of matrices increases.\n",
    "![](chol10x10-CPU.png)\n",
    "\n",
    "Running this on the GPU shows the dramatic slowdown with the looped version. It is scaling linearly looped but it is running in constant time for the batched version.\n",
    "![](chol10x10-GPU.png)\n",
    "\n",
    "Increasing the matrix size to 100 x 100 causes a flip in looped vs batched runtime on the CPU. The looped version is increasing in time but it's hared to see in the plot because the batched version is running very slow.\n",
    "![](chol100x100-CPU.png)\n",
    "\n",
    "**This is one of the most interesting plots.** Running on GPU with the 100 x100 matrices shows clearly the crossover point around 4000 matrices. The batched version is running in constant time and the looped version is scaling linearly. That trend line continues. Even at 1 million matrices the batched version on GPU takes about 7.5 seconds. It's independent of the number of matrices in the batch! The looped version would be running for a very long time.\n",
    "![](chol100x100-GPU.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to the QM code ...\n",
    "\n",
    "---\n",
    "\n",
    "## Code Optimization: From loops and matrix operations to batched tensor operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A surprise code block that used a lot of run time\n",
    "This is a real simple chunk of code. It just copies the upper triangle of H and S to the lower part.  It's only executed one time before the energy is computed but look at the time it took!  (on GPU)\n",
    "```\n",
    "# Timing\n",
    "    start_time = time.time()    \n",
    "    # complete lower triangle of H and S\n",
    "    for i in range(0,nb):\n",
    "        for j in range(i+1,nb):\n",
    "            H[j,i] = H[i,j]\n",
    "            S[j,i] = S[i,j]\n",
    "# End-Timing    \n",
    "    print(\" {:20} {:.6f} seconds \".format('Complete H', time.time() - start_time))\n",
    "```\n",
    "##### Complete H and S, took 5.018198 seconds\n",
    "\n",
    "Now the same thing but using the PyTorch triu() function to get rid of the loops,\n",
    "```\n",
    "# Timing\n",
    "    start_time = time.time()    \n",
    "    # complete lower triangle of H and S\n",
    "    H = th.triu(H,1)+th.t(th.triu(H))\n",
    "    S = th.triu(S,1)+th.t(th.triu(S))\n",
    "# End-Timing    \n",
    "```\n",
    "##### Complete H and S,  took  0.000117 seconds 42888 times faster!\n",
    "\n",
    "That was the biggest surprise of all of the code optimizations that I did. This is on the GPU ... **conclusion avoid unnecessary loops on the GPU!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructuring the code (inner loop elimination)\n",
    "The code in Part 2) was mostly contained in two functions, one that computed \"matrix elements over basis functions\" and another function that looped over that \"matrix element\" code to generate the larger matrices H and S that are used to compute the energy of a system. The function \"matrix_elements()\" will be rolled into the \"energy()\" function by eliminating some of the loops in energy().\n",
    "\n",
    "I'll strip out some of the code and comments. The matrix_elements() function will be removed and rolled into energy() like this,\n",
    "#### From-this: (loop over matrix terms)\n",
    "```\n",
    "def energy(x,n,nb,Mass,Charge,Sym,symc):\n",
    "    ...\n",
    "    # outer loop is over symmetry terms\n",
    "    for k in range(0,nsym):\n",
    "\n",
    "        for j in range(0,nb):\n",
    "            for i in range(j,nb):\n",
    "\n",
    "                vechLi = X[i,:]\n",
    "                vechLj = X[j,:]\n",
    "\n",
    "                matels = matrix_elements(n,vechLi,vechLj,Sym[:,:,k],Mass,Charge);\n",
    "\n",
    "                S[i,j] += symc[k]*matels['skl'];\n",
    "                T[i,j] += symc[k]*matels['tkl'];\n",
    "                V[i,j] += symc[k]*matels['vkl'];\n",
    "\n",
    "    H = T + V\n",
    "\n",
    "    # The energy from the Rayleigh quotient\n",
    "\n",
    "    cHc = c@H@c;\n",
    "    cSc = c@S@c;\n",
    "    eng = cHc/cSc;\n",
    "\n",
    "    return eng\n",
    "```\n",
    "#### To-this: (compute matrices all-at-once, removing 2 inner loops)\n",
    "\n",
    "```\n",
    "def b_energyrc(x,n,nb,Mass,Qmat,Sym,symc):\n",
    "    ...\n",
    "    # outer loop is over symmetry terms, the matrices are summed over these sym terms\n",
    "    for k in range(0,nsym):\n",
    "\n",
    "        # MATRIX ELEMENTS\n",
    "\n",
    "        # Overlap: (normalized)\n",
    "        # Skl = 2^3n/2 (||Lk|| ||Ll||/|AKL|)^3/2\n",
    "        SKL = 2**(n*1.5) * th.sqrt( th.pow(th.ger(detL, detL)/detAKL ,3) );\n",
    "\n",
    "        # Kinetic energy\n",
    "        #TKL = SKL*(6*th.trace(Mass@Ak@invAkl@Al)) = skl*(6*th.sum(Mass*(Ak@invAkl@Al)))\n",
    "\n",
    "        Tmat = th.zeros_like(invAKL)\n",
    "        Tmat = th.matmul(th.transpose(AK.repeat((nb,1,1,1)), 0,1), th.matmul(invAKL,AL))\n",
    "        TKL = 6*SKL*th.sum(Mass*Tmat, dim=(-2,-1))\n",
    "\n",
    "        # potential energy\n",
    "        TWOoSqrtPI = 1.1283791670955126 # 2/sqrt(pi)\n",
    "\n",
    "        VKL = TWOoSqrtPI*SKL*th.sum(RIJ*Qmat, dim=(-2,-1))\n",
    "\n",
    "        # accumulate matrices over sym terms\n",
    "        S = S + symc[k]*SKL\n",
    "        T = T + symc[k]*TKL\n",
    "        V = V + symc[k]*VKL\n",
    "\n",
    "    # Hamiltonian\n",
    "    H = T + V\n",
    "\n",
    "    # compute Rayleigh quotient (it is the smallest energy eigen value when minimized over c)\n",
    "    cHc = c@H@c;\n",
    "    cSc = c@S@c;\n",
    "    eng = cHc/cSc;\n",
    "\n",
    "    return eng           \n",
    "```\n",
    "---\n",
    "\n",
    "### Optimized code snippets\n",
    "\n",
    "We wont go over every change made in the code but we will look at a few code blocks to illustrate important ideas.\n",
    "\n",
    "#### From-this: (individual arrays of parameters and formula terms)\n",
    "```\n",
    "for j in range(0,nb):\n",
    "    for i in range(j,nb):\n",
    "\n",
    "        vechLi = X[i,:]\n",
    "        vechLj = X[j,:]\n",
    "\n",
    " --> to matrix_element()\n",
    "        # build Lk and Ll\n",
    "\n",
    "        Lk = vech2L(vechLk,n);\n",
    "        Ll = vech2L(vechLl,n);\n",
    "\n",
    "        # apply symmetry projection on Ll\n",
    "\n",
    "        # th.t() is shorthand for th.transpose(X, 0,1)\n",
    "        PLl = th.t(Sym) @ Ll;\n",
    "\n",
    "        # build Ak, Al, Akl, invAkl\n",
    "\n",
    "        Ak = Lk@th.t(Lk);\n",
    "        Al = PLl@th.t(PLl);\n",
    "        Akl = Ak+Al\n",
    "        invAkl = th.inverse(Akl);\n",
    "```\n",
    "#### To-this: (the complete set of parameter structures and formula terms)\n",
    "```\n",
    "# generate tensor of lower triangular matrices from X\n",
    "# these are the non-linear parameters of the basis set\n",
    "L = th.zeros((nb,n,n), device=device, dtype=dtype)\n",
    "L = bvech2L(X,nb,n)\n",
    "```\n",
    "Note the use of \"bvech2L()\" that is a batched version of that code. It generates all of the nb (number of basis functions) lower triangle matrices Lk and puts them in L an (nb x n x n) tensor -- a stack of nb, n x n matrices.\n",
    "```\n",
    "# get the determinates for L |L| is the product of diag elements\n",
    "detL = th.abs(th.prod(th.diagonal(L, offset=0, dim1=-1, dim2=-2),1))\n",
    "```\n",
    "Get determinants of all Lk in 1 tensor operation. Those are numpy like broadcasting operations specified by the \"dim\" attributes.\n",
    "```\n",
    "# create the tensor of matrix products of the L matrices AKL = L x Ltranspose\n",
    "AK = th.matmul(L,th.transpose(L, 1, 2))\n",
    "```\n",
    "Take the matrix product of all Lk matrices at once to make the Ak matrices. the tensor AK has the same shape as L.\n",
    "```\n",
    "--> to k (symmetry) loop\n",
    "    P = Sym[k,:,:]\n",
    "    # symetry projection is applied only to \"ket\" this constructs AL\n",
    "    AL = th.matmul(th.t(P), th.matmul(AK,P))\n",
    "```\n",
    "Multiply all of the matrices in AK by the symmetry term P at the same time to make AL. AL=P'AKP for all AK. This is numpy like broadcasting again.\n",
    "```\n",
    "    # Akl = Ak + Al\n",
    "    AKL = th.zeros((nb,nb,n,n), device=device, dtype=dtype)\n",
    "    #for i in range(nb):\n",
    "    #    for j in range(nb):\n",
    "    #        #AKL[i,j] =  th.add(AK[i], AL[j])\n",
    "    #        AKL[i,j] =  AK[i] + AL[j]\n",
    "    AKL = AL.repeat((nb,1,1,1)) + th.transpose(AK.repeat((nb,1,1,1)), 0,1)\n",
    "```\n",
    "This bit of code is a little tricky so I left my first tensor code attempt commented out since it is easier to see what is happening there.  AKL is an \"outer-sum\" of all combinations of the elements in AK and AL. You can see that in the loops.  The code I ended up with eliminates those loops by essentially repeating all of the AL and AK terms in the shape of nb \"vectors of matrices\" and then summing them to get the \"outer sum\".  The result is the tensor AKL of dimenson (nb x nb x n x n). This is \"tiling\" all of the n x n matrices as sums of each other. Think of AKL as an nb x nb matrix of n x n matrices! This tensor structure works seamlessly with my batched algorithms for Cholesky decomposition and inverse below. That \"repeat\" construct is a powerful idea to eliminate loops.\n",
    "```\n",
    "    # get the Cholesky decomp of all Akl martices\n",
    "    cholAKL = cholesky(AKL)\n",
    "\n",
    "    # get determinates of AKL from diags |Akl|= |Lk|**2\n",
    "    detAKL = th.prod(th.diagonal(cholAKL, offset=0, dim1=-1, dim2=-2),-1)**2\n",
    "\n",
    "    # compute inverses of lower tringular matrices in cholAKL\n",
    "    invLKL = inverseL(cholAKL)\n",
    "\n",
    "    # inverses Akl^-1 = Lkl' x Lkl\n",
    "    invAKL = th.matmul(th.transpose(invLKL, dim0=-1, dim1=-2),invLKL)\n",
    "```\n",
    "Those functions above compute all of their respective terms all-at-once! For real job runs there can be on the order of a million terms i.e. (nb x nb) = (1000 x 1000) so this gives a huge speedup.\n",
    "\n",
    ">**The big magic is that on the Titan V GPU, with batched tensor algorithms, those million terms are all computed in the same time it would take to compute 1!!!**\n",
    "\n",
    " **This is how I was able to take a completely reasonable looking implementation of the code and make it 16000 times faster on the Titan V GPU!**\n",
    "\n",
    "### Program speedup (16452 Times Faster)\n",
    "Remember the \"teaser\" I gave in Part 2)? Here it is again. In the next section I will present the code that produced that huge speedup.\n",
    "```\n",
    "start_time = time.time()\n",
    "xrestart = opt_energy(steps=1, num_basis=512)\n",
    "print(\" took {} seconds \".format(time.time() - start_time))\n",
    "```\n",
    "```\n",
    "step:     0  f: -0.720436686340  gradNorm: 4.060212842\n",
    " took 2768.9312148094177 seconds\n",
    "```\n",
    "2768 seconds for one step is too long!\n",
    "\n",
    "(Note: these results are from random starting points so they differ greatly. )\n",
    "```\n",
    "start_time = time.time()\n",
    "xrestart = opt_energyrc(steps=1,num_basis=512)\n",
    "print(\" took {:.4f} seconds \".format(time.time() - start_time))\n",
    "```\n",
    "\n",
    "```\n",
    "step:     0  f: 82.210051671516  gradNorm: 495.193096479\n",
    " took 0.1683 seconds\n",
    "```\n",
    "**Yes!, from 2768 seconds down to 0.1683 seconds! That's 16452 Times Faster!!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Complete Optimized Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution device:  cuda:0\n",
      "PyTorch version:  1.0.1\n",
      "CUDA available:  True\n",
      "CUDA version:  10.0\n",
      "CUDA device: GeForce GTX 1060 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "dtype = th.float64\n",
    "\n",
    "gpuid = 0\n",
    "device = th.device(\"cuda:\"+ str(gpuid))\n",
    "#device = th.device(\"cpu\")\n",
    "\n",
    "print(\"Execution device: \",device)\n",
    "print(\"PyTorch version: \", th.__version__ )\n",
    "print(\"CUDA available: \", th.cuda.is_available())\n",
    "print(\"CUDA version: \", th.version.cuda)\n",
    "print(\"CUDA device:\", th.cuda.get_device_name(gpuid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def vech2L(v,n):\n",
    "    count = 0\n",
    "    L = th.zeros((n,n), device=device, dtype=dtype)\n",
    "    for j in range(n):\n",
    "        for i in range(j,n):\n",
    "            L[i,j]=v[count]\n",
    "            count = count + 1\n",
    "    return L \n",
    "\n",
    "# batched vech2L input is \"X\" as V nb x n(n+1)/2\n",
    "def bvech2L(V,nb,n):\n",
    "    count = 0\n",
    "    L = th.zeros((nb,n,n), device=device, dtype=dtype)\n",
    "    for j in range(n):\n",
    "        for i in range(j,n):\n",
    "            L[...,i,j]=V[...,count]\n",
    "            count = count + 1\n",
    "    return L + th.eye(n, device=device, dtype=dtype)\n",
    "    #return L\n",
    "\n",
    "# Batched Cholesky decomp\n",
    "def cholesky(A):\n",
    "    L = th.zeros_like(A)\n",
    "    \n",
    "    for i in range(A.shape[-1]):\n",
    "        for j in range(i+1):\n",
    "            s = 0.0\n",
    "            for k in range(j):\n",
    "                s = s + L[...,i,k].clone() * L[...,j,k].clone()\n",
    "            \n",
    "            L[...,i,j] = th.sqrt(A[...,i,i] - s) if (i == j) else \\\n",
    "                      (1.0 / L[...,j,j].clone() * (A[...,i,j] - s))\n",
    "    return L\n",
    "\n",
    "# Batched inverse of lower triangular matrices \n",
    "def inverseL(L):\n",
    "    n = L.shape[-1]\n",
    "    invL = th.zeros_like(L)\n",
    "    for j in range(0,n):\n",
    "        invL[...,j,j] = 1.0/L[...,j,j]\n",
    "        for i in range(j+1,n):\n",
    "            S = 0.0\n",
    "            for k in range(i+1):\n",
    "                S = S - L[...,i,k]*invL[...,k,j].clone()\n",
    "            invL[...,i,j] = S/L[...,i,i]\n",
    "\n",
    "    return invL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_energyrc(x,n,nb,Mass,Qmat,Sym,symc):\n",
    "    \n",
    "    nx = len(x);\n",
    "    nn = int(n*(n+1)/2);\n",
    "    nsym = len(symc);\n",
    "    \n",
    "    # extract linear coefs \"eigen vector\"\n",
    "    c = x[-nb:];\n",
    "    # reshape non-linear variables for easier indexing\n",
    "    X = th.reshape(x[:nb*nn], (nb,nn))\n",
    "    \n",
    "    # generate tensor of lower triangular matrices from X\n",
    "    # these are the non-linear parameters of the basis set\n",
    "    L = th.zeros((nb,n,n), device=device, dtype=dtype)\n",
    "    L = bvech2L(X,nb,n)\n",
    "    \n",
    "    # get the determinates for L |L| is the product of diag elements\n",
    "    detL = th.abs(th.prod(th.diagonal(L, offset=0, dim1=-1, dim2=-2),1))\n",
    "    \n",
    "    # create the tensor of matrix products of the L matrices AKL = L x Ltranspose\n",
    "    AK = th.matmul(L,th.transpose(L, 1, 2))\n",
    "\n",
    "    \n",
    "    # Initialize H T V and S matrices\n",
    "    # H = T + V, we are solving (H-ES)c = 0 for E (energy)\n",
    "    H = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    S = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    T = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    V = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    \n",
    "\n",
    "    # outer loop is over symmetry terms, the matrices are summed over these sym terms\n",
    "    for k in range(0,nsym):\n",
    "        \n",
    "        P = Sym[k,:,:]\n",
    "        # symetry projection is applied only to \"ket\" this constructs AL\n",
    "        AL = th.matmul(th.t(P), th.matmul(AK,P))\n",
    "\n",
    "        # Akl = Ak + Al\n",
    "        AKL = th.zeros((nb,nb,n,n), device=device, dtype=dtype)\n",
    "        #for i in range(nb):\n",
    "        #    for j in range(nb):\n",
    "        #        #AKL[i,j] =  th.add(AK[i], AL[j])\n",
    "        #        AKL[i,j] =  AK[i] + AL[j]\n",
    "        AKL = AL.repeat((nb,1,1,1)) + th.transpose(AK.repeat((nb,1,1,1)), 0,1)\n",
    "        \n",
    "        # get the Cholesky decomp of all Akl martices\n",
    "        cholAKL = cholesky(AKL)\n",
    "        \n",
    "        # get determinates of AKL from diags |Akl|= |Lk|**2\n",
    "        detAKL = th.prod(th.diagonal(cholAKL, offset=0, dim1=-1, dim2=-2),-1)**2\n",
    "        \n",
    "        # compute inverses of lower tringular matrices in cholAKL\n",
    "        invLKL = inverseL(cholAKL)\n",
    "        \n",
    "        # inverses Akl^-1 = Lkl' x Lkl\n",
    "        invAKL = th.matmul(th.transpose(invLKL, dim0=-1, dim1=-2),invLKL)\n",
    "\n",
    "        # get terms needed for potential energy V\n",
    "        RIJ = th.zeros_like(invAKL, device=device, dtype=dtype);\n",
    "        # 1/rij i~=j\n",
    "        for j in range(0,n-1):\n",
    "            for i in range(j+1,n):\n",
    "                tmp2 = invAKL[...,i,i] + invAKL[...,j,j] - 2*invAKL[...,i,j];\n",
    "                RIJ[...,i,j] = th.rsqrt(tmp2)\n",
    "\n",
    "        # 1/rij i=j\n",
    "        for i in range(0,n):\n",
    "            RIJ[...,i,i] = th.rsqrt(invAKL[...,i,i])    \n",
    "\n",
    "        # MATRIX ELEMENTS\n",
    "        \n",
    "        # Overlap: (normalized)\n",
    "        # Skl = 2^3n/2 (||Lk|| ||Ll||/|AKL|)^3/2\n",
    "        SKL = 2**(n*1.5) * th.sqrt( th.pow(th.ger(detL, detL)/detAKL ,3) );\n",
    "\n",
    "        # Kinetic energy\n",
    "        #TKL = SKL*(6*th.trace(Mass@Ak@invAkl@Al)) = skl*(6*th.sum(Mass*(Ak@invAkl@Al)))\n",
    "\n",
    "        Tmat = th.zeros_like(invAKL)\n",
    "        #for i in range(nb):\n",
    "        #    for j in range(nb):\n",
    "        #        Tmat[i,j] = (AK[i]@invAKL[i,j]@AL[j])\n",
    "        Tmat = th.matmul(th.transpose(AK.repeat((nb,1,1,1)), 0,1), th.matmul(invAKL,AL))\n",
    "        TKL = 6*SKL*th.sum(Mass*Tmat, dim=(-2,-1))\n",
    "\n",
    "        # potential energy\n",
    "        TWOoSqrtPI = 1.1283791670955126 # 2/sqrt(pi)\n",
    "        \n",
    "        VKL = TWOoSqrtPI*SKL*th.sum(RIJ*Qmat, dim=(-2,-1))\n",
    "    \n",
    "        # accumulate matrices over sym terms\n",
    "        S = S + symc[k]*SKL\n",
    "        T = T + symc[k]*TKL\n",
    "        V = V + symc[k]*VKL\n",
    "        \n",
    "    # Hamiltonian\n",
    "    H = T + V\n",
    "    \n",
    "    # complete lower triangle of H and S\n",
    "    #for i in range(0,nb):\n",
    "    #    for j in range(i+1,nb):\n",
    "    #        H[j,i] = H[i,j]\n",
    "    #        S[j,i] = S[i,j]\n",
    "    #        #H[i,j] = H[j,i];\n",
    "    #        #S[i,j] = S[j,i];\n",
    "    H = th.triu(H,1)+th.t(th.triu(H))\n",
    "    S = th.triu(S,1)+th.t(th.triu(S))\n",
    "    # compute Rayleigh quotent (it is the smallest energy eigen value when minimized over c)\n",
    "    cHc = c@H@c;\n",
    "    cSc = c@S@c;\n",
    "    eng = cHc/cSc;\n",
    "    \n",
    "    return eng "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_energyrc(steps=1, num_basis=8, restart=True):\n",
    "    \n",
    "    #\n",
    "    # Li BO setup\n",
    "    #\n",
    "    n=3;\n",
    "    \n",
    "    Mass = th.tensor([[0.5, 0.0, 0.0],\n",
    "                     [0.0, 0.5, 0.0],\n",
    "                     [0.0, 0.0, 0.5]], device=device, dtype=dtype);\n",
    "    \n",
    "    Charge = th.tensor([-3, 1, 1, -3, 1, -3], device=device, dtype=dtype);\n",
    "    Charge = vech2L(Charge,n)\n",
    "    \n",
    "    # symmetry projection terms\n",
    "    Sym = th.zeros((6,3,3), device=device, dtype=dtype)\n",
    "    # (1)(2)(3)\n",
    "    Sym[0,:,:] = th.tensor([[1,0,0],[0,1,0],[0,0,1]], device=device, dtype=dtype);\n",
    "    # (12)\n",
    "    Sym[1,:,:] = th.tensor([[0,1,0],[1,0,0],[0,0,1]], device=device, dtype=dtype);\n",
    "    # (13)\n",
    "    Sym[2,:,:] = th.tensor([[0,0,1],[0,1,0],[1,0,0]], device=device, dtype=dtype);\n",
    "    # (23)\n",
    "    Sym[3,:,:] = th.tensor([[1,0,0],[0,0,1],[0,1,0]], device=device, dtype=dtype);\n",
    "    # (123)\n",
    "    Sym[4,:,:] = th.tensor([[0,1,0],[0,0,1],[1,0,0]], device=device, dtype=dtype);\n",
    "    # (132)\n",
    "    Sym[5,:,:] = th.tensor([[0,0,1],[1,0,0],[0,1,0]], device=device, dtype=dtype);\n",
    "\n",
    "    # coeff's\n",
    "    symc = th.tensor([4.0,4.0,-2.0,-2.0,-2.0,-2.0], device=device, dtype=dtype);\n",
    "\n",
    "    # Sample parameters should return energy of -7.3615\n",
    "    xvechL=th.tensor([\n",
    "         1.6210e+00, -2.1504e-01,  9.0755e-01,  9.7866e-01, -2.8418e-01,\n",
    "        -3.5286e+00, -3.3045e+00, -4.5036e+00, -3.2116e-01, -7.1901e-02,\n",
    "         1.5167e+00, -8.4489e-01, -2.1377e-01, -3.6127e-03, -5.3774e-03,\n",
    "        -2.1263e+00, -2.5191e-01,  2.1235e+00, -2.1396e-01, -1.4084e-03,\n",
    "        -1.0092e-02,  4.5349e+00,  9.4837e-03,  1.1225e+00, -2.1315e-01,\n",
    "         5.8451e-02, -4.9410e-03,  5.0853e+00,  7.3332e-01,  5.0672e+00,\n",
    "        -2.1589e-01, -6.8986e-03, -1.4310e-02,  1.5979e+00,  3.3946e-02,\n",
    "        -8.7965e-01, -1.1121e+00, -2.1903e-03, -4.6925e-02,  2.1457e-01,\n",
    "         3.3045e-03,  4.5120e+00, -2.1423e-01, -1.6493e-02, -2.3429e-03,\n",
    "        -8.6715e-01, -6.7070e-02,  1.5998e+00\n",
    "     ], device=device, dtype=dtype, requires_grad=False)\n",
    "\n",
    "    evec = th.tensor([\n",
    "      -6.0460e-02,  7.7708e-05, 1.6152e+00,  9.5443e-01,  \n",
    "      1.1771e-01,  3.2196e+00,  9.6344e-01, 3.1398e+00\n",
    "    ], device=device, dtype=dtype, requires_grad=False)\n",
    "\n",
    "    \n",
    "    # uncomment following lines to test above \n",
    "    #nb=8\n",
    "    #x1 = th.tensor(th.cat((xvechL,evec)), device=device, dtype=dtype, requires_grad=True)\n",
    "    #energy = b_energyrc(x1,n,nb,Mass,Charge,Sym,symc) \n",
    "    #print(energy) # should be -7.3615\n",
    "    #return x1\n",
    "    \n",
    "    if restart:\n",
    "        nb=num_basis\n",
    "        x1 = xrestart\n",
    "    else:\n",
    "        # random start point\n",
    "        nb=num_basis\n",
    "        #th.manual_seed(333)\n",
    "        x1 = th.empty(int(nb*n*(n+1)/2 + nb), device=device, dtype=dtype, requires_grad=True)\n",
    "        th.nn.init.uniform_(x1, a=-0.8, b=0.8)\n",
    "        \n",
    "    # start from a restart value\n",
    "    #x1 = xrestart\n",
    "    #print(energy)\n",
    "    #return x1\n",
    "    \n",
    "    # Do the Optimization\n",
    "    #optimizer = th.optim.LBFGS([x1])\n",
    "    #optimizer = th.optim.Adadelta([x1], lr=160.0)\n",
    "    #optimizer = th.optim.Adam([x1], lr=0.00005)\n",
    "    optimizer = th.optim.Rprop([x1], lr=0.0001, etas=(0.5, 1.2), step_sizes=(1e-07, 50))\n",
    "    \n",
    "    #scheduler = th.optim.lr_scheduler.ReduceLROnPlateau(optimizer,threshold=0.00001,cooldown=3, verbose=True,patience=2, factor=0.5)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        loss = b_energyrc(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "        loss.backward()\n",
    "        #def closure():\n",
    "        #    return b_energyrc(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "        #optimizer.step(closure)\n",
    "        optimizer.step()\n",
    "        #scheduler.step(loss)\n",
    "        \n",
    "        if (i<20 or not i%10):print('step: {:5}  f: {:4.12f}  gradNorm: {:.9f}'.format(i, loss, th.norm(x1.grad)))\n",
    "    # print last value\n",
    "    print('step: {:5}  f: {:4.12f}  gradNorm: {:.9f}'.format(i, loss, th.norm(x1.grad)))\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart: 0\n",
      "step:     0  f: -1.968185022292  gradNorm: 6.392709931\n",
      "step:     1  f: -1.980709927416  gradNorm: 6.377731304\n",
      "step:     2  f: -1.995701872755  gradNorm: 6.359578720\n",
      "step:     3  f: -2.013637091345  gradNorm: 6.337628085\n",
      "step:     4  f: -2.035079584775  gradNorm: 6.311089921\n",
      "step:     5  f: -2.060694095767  gradNorm: 6.278849191\n",
      "step:     6  f: -2.091260082776  gradNorm: 6.239569133\n",
      "step:     7  f: -2.127686476203  gradNorm: 6.191613865\n",
      "step:     8  f: -2.171024622280  gradNorm: 6.132994643\n",
      "step:     9  f: -2.222476131145  gradNorm: 6.061188593\n",
      "step:    10  f: -2.283399639229  gradNorm: 5.973582806\n",
      "step:    11  f: -2.355302516237  gradNorm: 5.866628232\n",
      "step:    12  f: -2.439798864399  gradNorm: 5.735882620\n",
      "step:    13  f: -2.538541855461  gradNorm: 5.576159684\n",
      "step:    14  f: -2.653144127980  gradNorm: 5.383394134\n",
      "step:    15  f: -2.785080627756  gradNorm: 5.154144648\n",
      "step:    16  f: -2.935464363322  gradNorm: 4.884747181\n",
      "step:    17  f: -3.104618021457  gradNorm: 4.569937600\n",
      "step:    18  f: -3.291883142878  gradNorm: 4.212507576\n",
      "step:    19  f: -3.495385338432  gradNorm: 3.819046701\n",
      "step:    20  f: -3.711659334960  gradNorm: 3.397919527\n",
      "step:    30  f: -5.770783160177  gradNorm: 0.750257867\n",
      "step:    40  f: -7.035915376895  gradNorm: 0.266985856\n",
      "step:    50  f: -7.397588305619  gradNorm: 0.101524400\n",
      "step:    60  f: -7.460807740010  gradNorm: 0.029621026\n",
      "step:    70  f: -7.470092257812  gradNorm: 0.014830631\n",
      "step:    80  f: -7.473107102714  gradNorm: 0.011407767\n",
      "step:    90  f: -7.474085047476  gradNorm: 0.006754965\n",
      "step:    99  f: -7.474704617666  gradNorm: 0.004640343\n",
      " took 75.0446 seconds \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for i in range(1):\n",
    "    print(\"Optimization restart: {}\".format(i))\n",
    "    xrestart = opt_energyrc(steps=100,num_basis=128, restart=False)\n",
    "print(\" took {:.4f} seconds \".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You\n",
    "# Happy Computing! --dbk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.0",
   "language": "python",
   "name": "pytorch1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
